{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc839193-aba7-4ad9-ad31-1576b391d9d2",
   "metadata": {
    "id": "bc839193-aba7-4ad9-ad31-1576b391d9d2"
   },
   "source": [
    "# Fruit Detecion Hands-ON\n",
    "<br><img width=\"1024\" src=\"https://lacoro.org/assets/img/2025/banner_lightmode.png?t=1765113906926\">\n",
    "\n",
    "\n",
    "This activitiy looks to give an introducttory experience to precision agriculture using computer vision techniques.\n",
    "\n",
    "\n",
    "\n",
    "Objectives:\n",
    "- Undesrtand basic pipeline of training deep learning models.\n",
    "- Undesrtand difficulties present in agricultural environements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6YX_PM_TN-hM",
   "metadata": {
    "id": "6YX_PM_TN-hM"
   },
   "source": [
    "# In case of using colab\n",
    "Remember to connect to your drive and upload the dataset if you want to repeat several times  this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yEPmKHa1N7Yj",
   "metadata": {
    "id": "yEPmKHa1N7Yj"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38858c4-43b3-480d-b151-32659bb93ff1",
   "metadata": {
    "id": "d38858c4-43b3-480d-b151-32659bb93ff1"
   },
   "source": [
    "# How to start\n",
    "First we are gonna set up the dataset. Upload the dataset in your current folder.\n",
    "## Download the dataset\n",
    "\n",
    "\n",
    "In this occasion we are gonna use the dataset Cherry CO that contains detection and segmentation labels of cherries in agricultural environments:\n",
    "\n",
    "\n",
    "[Cherry-CO-dataset](https://drive.google.com/drive/u/0/folders/1_-sOfq6KC62i9_rKeMwwGba5uBOaagaj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa26ed-7fdb-4550-895d-48b22216122c",
   "metadata": {
    "id": "38fa26ed-7fdb-4550-895d-48b22216122c"
   },
   "outputs": [],
   "source": [
    "!unzip -o val.zip -d val_temp\n",
    "!unzip -o train.zip -d train_temp\n",
    "!mkdir datasets\n",
    "!mv val_temp/val datasets\n",
    "!mv train_temp/train datasets\n",
    "!rm -r val_temp\n",
    "!rm -r train_temp\n",
    "\n",
    "%cd datasets\n",
    "%cd val\n",
    "! unzip images.zip\n",
    "! unzip labels_yolo.zip\n",
    "!mv labels_ripeness labels\n",
    "\n",
    "%cd ..\n",
    "%cd train\n",
    "! unzip images.zip\n",
    "! unzip labels_yolo.zip\n",
    "!mv labels_ripeness labels\n",
    "%cd ..\n",
    "!mkdir labels\n",
    "!mkdir images\n",
    "\n",
    "!mv val/labels labels/val\n",
    "!mv val/images images/val\n",
    "\n",
    "!mv train/labels labels/train\n",
    "!mv train/images images/train\n",
    "\n",
    "%cd ..\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "itzcLZJIaUJW",
   "metadata": {
    "id": "itzcLZJIaUJW"
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics==8.3.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924be1c-3088-4994-916c-c80bb085f6f3",
   "metadata": {
    "id": "b924be1c-3088-4994-916c-c80bb085f6f3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "text_labels = \"\"\"\n",
    "path: ./ # dataset root dir\n",
    "train: images/train # train images (relative to 'path') 4 images\n",
    "val: images/val # val images (relative to 'path') 4 images\n",
    "test: # test images (optional)\n",
    "\n",
    "# Classes\n",
    "names:\n",
    "  0: ripe\n",
    "  1: unripe\n",
    "  2: green\n",
    "\"\"\"\n",
    "\n",
    "text_model = \"\"\"\n",
    "# Parameters\n",
    "nc: 3 # number of classes\n",
    "scales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n'\n",
    "  # [depth, width, max_channels]\n",
    "#  n: [0.50, 0.25, 1024] # summary: 181 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs\n",
    "  s: [0.50, 0.50, 1024] # summary: 181 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs\n",
    "#  m: [0.50, 1.00, 512] # summary: 231 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs\n",
    "#  l: [1.00, 1.00, 512] # summary: 357 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs\n",
    "#  x: [1.00, 1.50, 512] # summary: 357 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs\n",
    "# YOLO11n backbone|\n",
    "backbone:\n",
    "  # [from, repeats, module, args]\n",
    "  - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n",
    "  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n",
    "  - [-1, 2, C3k2, [256, False, 0.25]]\n",
    "  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n",
    "  - [-1, 2, C3k2, [512, False, 0.25]]\n",
    "  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n",
    "  - [-1, 2, C3k2, [512, True]]\n",
    "  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n",
    "  - [-1, 2, C3k2, [1024, True]]\n",
    "  - [-1, 1, SPPF, [1024, 5]] # 9\n",
    "  - [-1, 2, C2PSA, [1024]] # 10\n",
    "\n",
    "# YOLO11n head\n",
    "head:\n",
    "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
    "  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\n",
    "  - [-1, 2, C3k2, [512, False]] # 13\n",
    "\n",
    "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
    "  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\n",
    "  - [-1, 2, C3k2, [256, False]] # 16 (P3/8-small)\n",
    "\n",
    "  - [-1, 1, Conv, [256, 3, 2]]\n",
    "  - [[-1, 13], 1, Concat, [1]] # cat head P4\n",
    "  - [-1, 2, C3k2, [512, False]] # 19 (P4/16-medium)\n",
    "\n",
    "  - [-1, 1, Conv, [512, 3, 2]]\n",
    "  - [[-1, 10], 1, Concat, [1]] # cat head P5\n",
    "  - [-1, 2, C3k2, [1024, True]] # 22 (P5/32-large)\n",
    "\n",
    "  - [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)\n",
    "\"\"\"\n",
    "\n",
    "data_dict = yaml.safe_load(text_labels)\n",
    "model_dict = yaml.safe_load(text_model)\n",
    "with open('yolov11.yaml', 'w') as file:\n",
    "    yaml.dump(model_dict, file)\n",
    "\n",
    "with open('cherry.yaml', 'w') as file:\n",
    "    yaml.dump(data_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f0b8b-fd99-45a2-9224-0fbc18253342",
   "metadata": {
    "id": "045f0b8b-fd99-45a2-9224-0fbc18253342"
   },
   "source": [
    "# Understanding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681791b0-81a1-44e6-a4bb-16ac7f3b3528",
   "metadata": {
    "id": "681791b0-81a1-44e6-a4bb-16ac7f3b3528"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class YOLOUltralyticsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Ultralytics-style YOLO data.\n",
    "\n",
    "    Expects the following structure:\n",
    "        dataset/\n",
    "            images/set/*.jpg\n",
    "            labels/set/*.txt\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir=\"dataset\", split=\"train\", transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.img_dir = os.path.join(root_dir, \"datasets/images\", split)\n",
    "        self.lbl_dir = os.path.join(root_dir, \"datasets/labels\", split)\n",
    "\n",
    "        # list image files\n",
    "        self.img_files = sorted([\n",
    "            f for f in os.listdir(self.img_dir)\n",
    "            if f.lower().endswith((\".jpg\", \".JPG\", \".JPEG\", \".png\", \".jpeg\"))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "\n",
    "        # --- Load image ---\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        # --- Load label ---\n",
    "        label_name = os.path.splitext(img_name)[0] + \".txt\"\n",
    "        label_path = os.path.join(self.lbl_dir, label_name)\n",
    "\n",
    "        boxes = []\n",
    "        classes = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f.read().strip().splitlines():\n",
    "                    if line.strip():\n",
    "                        cls, cx, cy, w, h = map(float, line.split())\n",
    "                        boxes.append([cx, cy, w, h])\n",
    "                        classes.append([cls])\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # --- Apply transforms ---\n",
    "        if self.transform:\n",
    "            image = self.transform(image,boxes)\n",
    "        labels = {'bbox': boxes,'cat':torch.tensor(classes,dtype=torch.int64)}\n",
    "        return image, labels\n",
    "\n",
    "dataset = YOLOUltralyticsDataset(data_dict['path'],split='val')\n",
    "img,labels = dataset[135]\n",
    "print(labels)\n",
    "plt.imshow(img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf903ac-0833-4e46-aafb-a1173bdac7a2",
   "metadata": {
    "id": "bdf903ac-0833-4e46-aafb-a1173bdac7a2"
   },
   "outputs": [],
   "source": [
    "def draw_box(img,single_box,color=(),thickness=3):\n",
    "    bbox = single_box * torch.tile(torch.flip(torch.tensor(img.shape)[:2],dims=[0]),dims=[2])\n",
    "    bbox[:2] -= bbox[2:]/2\n",
    "    bbox[2:] += bbox[:2]\n",
    "    bbox = bbox.detach()\n",
    "    print(bbox)\n",
    "    img = cv2.rectangle(img.copy(), (int(bbox[0].item()), int(bbox[1])),(int(bbox[2]), int(bbox[3])),\n",
    "                                color,\n",
    "                                thickness=thickness)\n",
    "    return img\n",
    "\n",
    "img = draw_box(img,labels['bbox'][0],color=(0.0,0.0,255.0))\n",
    "plt.imshow(img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d8a1c-9e59-4fab-a63c-b63e34669f2a",
   "metadata": {
    "id": "102d8a1c-9e59-4fab-a63c-b63e34669f2a"
   },
   "source": [
    "## Understanting Bounding boxes\n",
    "Use the draw box function to draw every cherry based on their ripeness level (0 is red, 1 is orange and 2 is green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e06d9-faa5-40a8-bd18-930bb2affbd0",
   "metadata": {
    "id": "c25e06d9-faa5-40a8-bd18-930bb2affbd0"
   },
   "outputs": [],
   "source": [
    "def draw_functions(img,labels):\n",
    "    ## CODE HERE\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d49ed7-9681-448a-9dd7-7b64d710c77a",
   "metadata": {
    "id": "c3d49ed7-9681-448a-9dd7-7b64d710c77a"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dFPp69z3nJrz",
   "metadata": {
    "id": "dFPp69z3nJrz"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404669a-6c06-4cc6-965f-2a447db2cd67",
   "metadata": {
    "id": "0404669a-6c06-4cc6-965f-2a447db2cd67"
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov11.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a7c38-d1a7-4222-9f7c-57f760688bc5",
   "metadata": {
    "id": "f65a7c38-d1a7-4222-9f7c-57f760688bc5"
   },
   "outputs": [],
   "source": [
    "imgsz = 1080 # Changed from 540 to 640\n",
    "results = model.train(data='./cherry.yaml',\n",
    "                      batch=8,\n",
    "                      pretrained=False,\n",
    "                      epochs=6,\n",
    "                      single_cls=False,\n",
    "                      imgsz=imgsz, #\n",
    "                      lr0=0.0001, # 0.001, 0.00001\n",
    "                      optimizer='Adam', # Adam, SGD, AdamW, NAdam, RAdam, RMSProp\n",
    "                      device=0,\n",
    "                      project='runs',\n",
    "                      multi_scale=True\n",
    "\n",
    "                     )\n",
    "\n",
    "\n",
    "print(\"Class indices with average precision:\", results.ap_class_index)\n",
    "print(\"Average precision for all classes:\", results.box.all_ap)\n",
    "print(\"Average precision:\", results.box.ap)\n",
    "print(\"Average precision at IoU=0.50:\", results.box.ap50)\n",
    "print(\"Class indices for average precision:\", results.box.ap_class_index)\n",
    "print(\"Class-specific results:\", results.box.class_result)\n",
    "print(\"F1 score:\", results.box.f1)\n",
    "print(\"F1 score curve:\", results.box.f1_curve)\n",
    "print(\"Overall fitness score:\", results.box.fitness)\n",
    "print(\"Mean average precision:\", results.box.map)\n",
    "print(\"Mean average precision at IoU=0.50:\", results.box.map50)\n",
    "print(\"Mean average precision at IoU=0.75:\", results.box.map75)\n",
    "print(\"Mean average precision for different IoU thresholds:\", results.box.maps)\n",
    "print(\"Mean results for different metrics:\", results.box.mean_results)\n",
    "print(\"Mean precision:\", results.box.mp)\n",
    "print(\"Mean recall:\", results.box.mr)\n",
    "print(\"Precision:\", results.box.p)\n",
    "print(\"Precision curve:\", results.box.p_curve)\n",
    "print(\"Precision values:\", results.box.prec_values)\n",
    "print(\"Specific precision metrics:\", results.box.px)\n",
    "print(\"Recall:\", results.box.r)\n",
    "print(\"Recall curve:\", results.box.r_curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f2a9c2-8dff-4f59-9588-7eef9b4faca8",
   "metadata": {
    "id": "b9f2a9c2-8dff-4f59-9588-7eef9b4faca8"
   },
   "outputs": [],
   "source": [
    "\n",
    "main_path = 'runs/train22/'\n",
    "\n",
    "path_file_val = '{:s}/results.csv'.format(main_path)\n",
    "\n",
    "columns = ['epoch','time','train/box_loss','train/cls_loss','train/dfl_loss','metrics/precision(B)','metrics/recall(B)','metrics/mAP50(B)',\n",
    "               'metrics/mAP50-95(B)','val/box_loss','val/cls_loss','val/dfl_loss','lr/pg0','lr/pg1','lr/pg2']\n",
    "\n",
    "data = pd.read_csv(path_file_val,delimiter=',',skiprows=1,names=columns,index_col=False,\n",
    "                  dtype={c:float for c in columns})\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OT4frujuzUKz",
   "metadata": {
    "id": "OT4frujuzUKz"
   },
   "outputs": [],
   "source": [
    "\n",
    "metric_train = 'metrics/mAP50-95(B)' #\n",
    "\n",
    "plt.plot(data.index,data[metric_train],label=metric_train)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(metric_train)\n",
    "print(\"max value: \",data[metric_train].max())\n",
    "print(\"max index: \",data[metric_train].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q5Ih4auS0ZSv",
   "metadata": {
    "id": "Q5Ih4auS0ZSv"
   },
   "outputs": [],
   "source": [
    "\n",
    "metric_train = 'metrics/mAP50(B)' #\n",
    "\n",
    "\n",
    "plt.plot(data.index,data[metric_train],label=metric_train)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(metric_train)\n",
    "print(\"max value: \",data[metric_train].max())\n",
    "print(\"max index: \",data[metric_train].argmax())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19bcdf-2daf-4b0e-a887-6c8605bba61c",
   "metadata": {
    "id": "da19bcdf-2daf-4b0e-a887-6c8605bba61c"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1011f-bc68-4a4b-8a3f-9fbeb4017333",
   "metadata": {
    "id": "0bf1011f-bc68-4a4b-8a3f-9fbeb4017333"
   },
   "outputs": [],
   "source": [
    "idx = 125\n",
    "im1,labels = dataset[idx]\n",
    "results = model([im1], imgsz=imgsz,verbose=False)  # batch of images\n",
    "def plot_bboxes(results,threshold=0.4,use_labels=False,thickness=2):\n",
    "    img = results[0].orig_img[:,:,::-1] # original image\n",
    "    names = results[0].names # class names dict\n",
    "    scores = results[0].boxes.conf.cpu().numpy() # probabilities\n",
    "    classes = results[0].boxes.cls.cpu().numpy() # predicted classes\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy().astype(np.int32) # bboxes\n",
    "    n_classes = len(results[0].names)\n",
    "    colors_valid = [(255,200,0), (150,255,0),(255,0,0),(255,120,50),(0,255,0),(255,255,255),(0,0,0)]\n",
    "    colors={i:colors_valid[i] for i in range(n_classes)}\n",
    "    for score, cls, bbox in zip(scores, classes, boxes): # loop over all bboxes\n",
    "        class_label = names[cls] # class name\n",
    "        if threshold < score:\n",
    "            label = f\"{class_label} : {score:0.2f}\" # bbox label\n",
    "            lbl_margin = 3 #label margin\n",
    "            img = cv2.rectangle(img.copy(), (bbox[0], bbox[1]),(bbox[2], bbox[3]),\n",
    "                                colors[cls],\n",
    "                                thickness=thickness)\n",
    "            label_size = cv2.getTextSize(label, # labelsize in pixels\n",
    "                                         fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                         fontScale=1, thickness=thickness)\n",
    "            lbl_w, lbl_h = label_size[0] # label w and h\n",
    "            lbl_w += 2* lbl_margin # add margins on both sides\n",
    "            lbl_h += 2*lbl_margin\n",
    "\n",
    "            if use_labels:\n",
    "                img = cv2.rectangle(img, (bbox[0], bbox[1]), # plot label background\n",
    "                     (bbox[0]+lbl_w, bbox[1]-lbl_h),\n",
    "                     color=colors[cls],\n",
    "                     thickness=-1) # thickness=-1 means filled rectangle\n",
    "                cv2.putText(img, label, (bbox[0]+ lbl_margin, bbox[1]-lbl_margin), # write label to the image\n",
    "                            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            fontScale=1.0, color=(255, 255, 255 ),\n",
    "                            thickness=2)\n",
    "    return img\n",
    "img = plot_bboxes(results,use_labels=True,threshold=0.15) # plot annotated bboxes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8jbwA9xI4hRU",
   "metadata": {
    "id": "8jbwA9xI4hRU"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd05e5-0ced-4c6c-89d5-a1b2ac0c9400",
   "metadata": {
    "id": "16bd05e5-0ced-4c6c-89d5-a1b2ac0c9400"
   },
   "source": [
    "## Questions\n",
    "\n",
    "\n",
    "1. What are the main sources of false positives? what causes falses negatives?\n",
    "2. Can you notice any difference between pre-train and no pre-training?\n",
    "3. Given the next table what can you say about these hyper-parameters?\n",
    "\n",
    "    - Which is more relevant?\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29842beb-ef18-4b1e-857c-a2bd6cc3648f",
   "metadata": {
    "id": "29842beb-ef18-4b1e-857c-a2bd6cc3648f"
   },
   "outputs": [],
   "source": [
    "def calculate_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Calculate the IoU between two sets of bounding boxes.\n",
    "    boxes1: Tensor of shape (N, 4) where N is the number of boxes and 4 represents (x1, y1, x2, y2).\n",
    "    boxes2: Tensor of shape (M, 4) where M is the number of boxes and 4 represents (x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    # Calculate the areas of boxes1 and boxes2\n",
    "    areas1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    areas2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "\n",
    "    # Calculate the coordinates of the intersection boxes\n",
    "    x1_inter = torch.max(boxes1[:, None, 0], boxes2[:, 0])\n",
    "    y1_inter = torch.max(boxes1[:, None, 1], boxes2[:, 1])\n",
    "    x2_inter = torch.min(boxes1[:, None, 2], boxes2[:, 2])\n",
    "    y2_inter = torch.min(boxes1[:, None, 3], boxes2[:, 3])\n",
    "\n",
    "    # Calculate the areas of the intersection boxes\n",
    "    widths_inter = torch.clamp(x2_inter - x1_inter, min = 0)\n",
    "    heights_inter = torch.clamp(y2_inter - y1_inter, min = 0)\n",
    "    areas_inter = widths_inter * heights_inter\n",
    "\n",
    "    # Calculate the areas of the union boxes\n",
    "    areas_union = areas1[:, None] + areas2 - areas_inter\n",
    "\n",
    "    # Calculate the IoU\n",
    "    iou = areas_inter / areas_union\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "boxes_pred = results[0].boxes.xyxy.cpu()\n",
    "boxes_gt = labels['bbox'] * imgsz\n",
    "boxes_gt[:,:2] = boxes_gt[:,:2] - boxes_gt[:,2:]/2\n",
    "boxes_gt[:,2:] = boxes_gt[:,:2] + boxes_gt[:,2:]\n",
    "\n",
    "iou = calculate_iou(boxes_pred, boxes_gt)\n",
    "print(iou)\n",
    "best_results = torch.max(iou,dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4XKCyi8DOY27",
   "metadata": {
    "id": "4XKCyi8DOY27"
   },
   "source": [
    "## Analize Intersection over Union (IoU)\n",
    "\n",
    "- Analize some images and calculate average IoU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ab3d9-3d8d-4c60-ba02-2e7f1d265c7d",
   "metadata": {
    "id": "d18ab3d9-3d8d-4c60-ba02-2e7f1d265c7d"
   },
   "outputs": [],
   "source": [
    "## Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a029298-c3e2-4307-a3ce-86185f86b4a2",
   "metadata": {
    "id": "0a029298-c3e2-4307-a3ce-86185f86b4a2"
   },
   "source": [
    "## Video detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c6931-d8b5-4f03-b4df-50153c4b4fa8",
   "metadata": {
    "id": "a09c6931-d8b5-4f03-b4df-50153c4b4fa8"
   },
   "outputs": [],
   "source": [
    "video_file='short_video2.mov'\n",
    "cap = cv2.VideoCapture(video_file)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7aef1-96f4-421c-90cf-444cbf505004",
   "metadata": {
    "id": "cde7aef1-96f4-421c-90cf-444cbf505004"
   },
   "outputs": [],
   "source": [
    "is_read, frame = cap.read()\n",
    "\n",
    "if is_read:\n",
    "  shape = frame.shape\n",
    "  pred = model([frame[:shape[1],:shape[1]]])\n",
    "  frame = plot_bboxes(pred,use_labels=True,threshold=0.01) # plot annotated bboxes\n",
    "  plt.figure(figsize=(9,9))\n",
    "  plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4642b301-630e-4d95-bb69-2b7c9568bc7b",
   "metadata": {
    "id": "4642b301-630e-4d95-bb69-2b7c9568bc7b"
   },
   "source": [
    "## Questiones\n",
    "1. What type of inconsistencies can you see between succesive frames\n",
    "2. How would you go about improving this errors"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "track",
   "language": "python",
   "name": "track"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
